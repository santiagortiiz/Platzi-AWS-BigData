# Create, Configure and Execute ETL in AWS
- Connect Glue developer endpoint with local Zeppelin

# Zeppelin code based on AWS guide
- Specify the interpreter in the 1st line, example: %pyspark
- create GlueContext
- check connection with the data source with the interpreter statements

# ETL Job (transform)
- Go to Glue/ETL/Job
- create job
- select interpreter
- provide script stored in S3

# ETL Script
- GlueContext
- Catalog (DB & tables)
- Output directories
- Create Dynamic Frames                             # "Dataframes" of AWS

## Transform operations
- see AWS Glue ETL guide

## Run Job
- Write the ETL script in the Job
- Save the job
- Run the job

# Result of the Job
- Processed data must be uploaded to an S3 Bucket

# Add the processed data to the Glue Catalog with a Crawler
- Create and configure a Crawler that points to the processed data
- Create new database for the processed data
- Run the crawler to bring the data to the Glue Catalog



